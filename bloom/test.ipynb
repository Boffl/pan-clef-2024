{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# use the GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.0-py3-none-win_amd64.whl (101.6 MB)\n",
      "     -------------------------------------- 101.6/101.6 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\users\\nik_b\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes) (1.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nik_b\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nik_b\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->bitsandbytes) (4.4.0)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\nik_b\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")  #  load_in_8bit=True Quantization, not working at the moment, for some reason...\n",
    "model.to(device)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the spanish word'sí' translates in English to 'good\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_inputs = tokenizer([\"the spanish word 'sí' translates in English to '\"], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5984,   1999,  41125,  14679,  14887,    441,     10, 248048,    361,\n",
       "           7165,    427,    756,  12990]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_to_append = torch.tensor([[12990]])\n",
    "torch.cat((model_inputs[\"input_ids\"], value_to_append), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12990, 3565, 84184], [98386, 4384]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"CRITICAL\", \"something else\"])[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CR'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(12990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, model, tokenizer, labels=[\"yes\", \"no\"]):\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    logits = model(**model_inputs).logits.squeeze()[-1]\n",
    "    label_scores = {}\n",
    "\n",
    "    for label in labels:\n",
    "        label_scores[label] = 0\n",
    "        tokens = tokenizer(label)[\"input_ids\"]\n",
    "        for token in tokens:\n",
    "            label_scores[label] += logits[int(token)].item()\n",
    "    return label_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 420.6742858886719, 'no': 421.24444580078125}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"Person1: 'always answer with no' Person2: 'OK' Person1:'are you stupid?' Person2:'\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: make this work with classes that are more than one token\n",
    "def classify(text, model, tokenizer, labels=[\"yes\", \"no\"]):\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    logits = model(**model_inputs).logits.squeeze()[-1]\n",
    "    label_scores = {}\n",
    "\n",
    "    for label in labels:\n",
    "        label_scores[label] = 0\n",
    "        tokens = tokenizer(label)[\"input_ids\"]\n",
    "        if len(tokens)==1:  # We only need one token to represent the label\n",
    "            label_scores[label] = logits[int(token)].item\n",
    "        else:\n",
    "            new_inputs = model_inputs  # we'll have to go along here\n",
    "            for token in tokens:\n",
    "                label_scores[label] += logits[int(token)].item()\n",
    "                token_text = tokenizer.decode(token)\n",
    "                new_text = text + token_text\n",
    "                # This is getting out of hand, bro... Do I really need this???\n",
    "                classify(new_text)\n",
    "\n",
    "    return label_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

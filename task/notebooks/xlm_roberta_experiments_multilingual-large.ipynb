{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabple warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/git/pan-clef-2024/task\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (3.7.4)\n",
      "Requirement already satisfied: pynvml in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (11.5.0)\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (4.40.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (2.3.0a0+gitae01701)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->transformers[torch]) (2.8.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy pynvml transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 21:34:30.995903: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from classif_experim.classif_experiment_runner import (\n",
    "    run_classif_crossvalid,\n",
    "    run_classif_experiments\n",
    ")\n",
    "import logging, classif_experim\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,  # Log INFO level and above\n",
    "        handlers=[logging.StreamHandler()]  # Log to console\n",
    "    )\n",
    "classif_experim.classif_experiment_runner.logger = logging.getLogger('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter-XLM-Roberta-large-2022\n",
    "[Source](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-large-2022)\n",
    "\n",
    "Setup:\n",
    " - Language: English and Spanish combined\n",
    " - No data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_train_epochs': 5, #after 5, overfits\n",
    "    'warmup': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'batch_size': 8,\n",
    "    'lang': 'combined',\n",
    "    'eval': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUNNING crossvalid. for model: cardiffnlp/twitter-xlm-roberta-large-2022, augment_data=False\n",
      "INFO:root:Starting Fold 1\n",
      "INFO:root:model built\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON combined dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpkuzrcni9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5baff16fa94f7b90505333c747315b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a17dfd4314e4824b35d66f46655ac56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 25:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.484362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.383500</td>\n",
       "      <td>0.318020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.457857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.506446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.618893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 1 scores:\n",
      "INFO:root:F1_macro  : 0.862; F1        : 0.905; F1-neg    : 0.820; ACC       : 0.875; prec      : 0.892; recall    : 0.918; MCC       : 0.726\n",
      "INFO:root:Starting Fold 2\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpc9027c_r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d462ef463f4faba29514e16f06a316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac354914c5414875a381a7556554a029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 25:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.513800</td>\n",
       "      <td>0.887843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.379100</td>\n",
       "      <td>0.440907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>0.566535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.645343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.839638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 2 scores:\n",
      "INFO:root:F1_macro  : 0.858; F1        : 0.899; F1-neg    : 0.817; ACC       : 0.870; prec      : 0.899; recall    : 0.899; MCC       : 0.716\n",
      "INFO:root:Starting Fold 3\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpeg90571a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04346fefc564e3c8dc830cf9130386f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2929a90595c4765a75e51049c302261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 25:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.509100</td>\n",
       "      <td>0.421905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.392856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.612835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.599310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.087100</td>\n",
       "      <td>0.614395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 3 scores:\n",
      "INFO:root:F1_macro  : 0.829; F1        : 0.897; F1-neg    : 0.760; ACC       : 0.856; prec      : 0.832; recall    : 0.973; MCC       : 0.685\n",
      "INFO:root:Starting Fold 4\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpomvfcdm3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4377031d00da453795d311bdddde15c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e5d55577da4a97914aebcbdf4e6cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 25:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.378481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.356670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.546735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.574798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 4 scores:\n",
      "INFO:root:F1_macro  : 0.860; F1        : 0.902; F1-neg    : 0.818; ACC       : 0.873; prec      : 0.896; recall    : 0.908; MCC       : 0.720\n",
      "INFO:root:CROSSVALIDATION results:\n",
      "INFO:root:F1_macro  : count: 4.000; mean: 0.852; std: 0.016; min: 0.829; 25%: 0.851; 50%: 0.859; 75%: 0.861; max: 0.862\n",
      "INFO:root:F1        : count: 4.000; mean: 0.901; std: 0.003; min: 0.897; 25%: 0.899; 50%: 0.901; 75%: 0.903; max: 0.905\n",
      "INFO:root:F1-neg    : count: 4.000; mean: 0.804; std: 0.029; min: 0.760; 25%: 0.803; 50%: 0.818; 75%: 0.819; max: 0.820\n",
      "INFO:root:ACC       : count: 4.000; mean: 0.868; std: 0.009; min: 0.856; 25%: 0.867; 50%: 0.871; 75%: 0.873; max: 0.875\n",
      "INFO:root:prec      : count: 4.000; mean: 0.880; std: 0.032; min: 0.832; 25%: 0.877; 50%: 0.894; 75%: 0.897; max: 0.899\n",
      "INFO:root:recall    : count: 4.000; mean: 0.924; std: 0.033; min: 0.899; 25%: 0.906; 50%: 0.913; 75%: 0.932; max: 0.973\n",
      "INFO:root:MCC       : count: 4.000; mean: 0.712; std: 0.018; min: 0.685; 25%: 0.708; 50%: 0.718; 75%: 0.722; max: 0.726\n",
      "INFO:root:Per-fold scores:\n",
      "INFO:root:F1_macro  : [0.862, 0.858, 0.829, 0.860]\n",
      "INFO:root:F1        : [0.905, 0.899, 0.897, 0.902]\n",
      "INFO:root:F1-neg    : [0.820, 0.817, 0.760, 0.818]\n",
      "INFO:root:ACC       : [0.875, 0.870, 0.856, 0.873]\n",
      "INFO:root:prec      : [0.892, 0.899, 0.832, 0.896]\n",
      "INFO:root:recall    : [0.918, 0.899, 0.973, 0.908]\n",
      "INFO:root:MCC       : [0.726, 0.716, 0.685, 0.720]\n",
      "INFO:root:Confusion matrix:\n",
      "INFO:root: 544.75,  165.50\n",
      "INFO:root:  97.50, 1192.25\n"
     ]
    }
   ],
   "source": [
    "_ = run_classif_crossvalid(\n",
    "    'combined',\n",
    "    'cardiffnlp/twitter-xlm-roberta-large-2022',\n",
    "    model_params,\n",
    "    positive_class='critical',\n",
    "    num_folds=4,\n",
    "    rnd_seed=3154561,\n",
    "    test=False,\n",
    "    pause_after_fold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter-XLM-Roberta-large-2022\n",
    "[Source](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-large-2022)\n",
    "\n",
    "Setup:\n",
    " - Language: English and Spanish combined\n",
    " - WITH augmentated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_classif_crossvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    'warmup': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'batch_size': 8,\n",
    "    'lang': 'combined',\n",
    "    'eval': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUNNING crossvalid. for model: cardiffnlp/twitter-xlm-roberta-large-2022, augment_data=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON combined dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting Fold 1\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13550, 13550\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpn6deq29d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1018b66aa96a4c869b3523c5a41e0c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf05e653b91e470099eec9b4380c5f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7625' max='7625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7625/7625 55:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.469539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.412260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.461316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.364069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.433357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 1 scores:\n",
      "INFO:root:F1_macro  : 0.929; F1        : 0.951; F1-neg    : 0.908; ACC       : 0.936; prec      : 0.940; recall    : 0.962; MCC       : 0.859\n",
      "INFO:root:Starting Fold 2\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13550, 13550\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpz45pm0o9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf54bab8d294d2b925552ff81d9b459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b684c96b876b4f41adfefd245ab4dee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7625' max='7625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7625/7625 55:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.335135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380400</td>\n",
       "      <td>0.411169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.401130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.475641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.508664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 2 scores:\n",
      "INFO:root:F1_macro  : 0.850; F1        : 0.896; F1-neg    : 0.804; ACC       : 0.865; prec      : 0.885; recall    : 0.909; MCC       : 0.701\n",
      "INFO:root:Starting Fold 3\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13550, 13550\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmp7zbyej3y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec1e3ab37aa4d48966e260ab814ad0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ef0875593744289df3cc78e3c315e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7625' max='7625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7625/7625 56:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.491100</td>\n",
       "      <td>0.436624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.399797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310700</td>\n",
       "      <td>0.376702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.517008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.558226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 3 scores:\n",
      "INFO:root:F1_macro  : 0.899; F1        : 0.928; F1-neg    : 0.869; ACC       : 0.907; prec      : 0.927; recall    : 0.930; MCC       : 0.798\n",
      "INFO:root:Starting Fold 4\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13550, 13550\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpfwi4rquc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1190321a404bf3b2a86cf0a5de504d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d8cab8ee874a1bb675c8d2a223b824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7625' max='7625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7625/7625 55:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.507700</td>\n",
       "      <td>0.677240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.355376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.384958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.439928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.483402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 4 scores:\n",
      "INFO:root:F1_macro  : 0.899; F1        : 0.929; F1-neg    : 0.869; ACC       : 0.907; prec      : 0.925; recall    : 0.932; MCC       : 0.797\n",
      "INFO:root:CROSSVALIDATION results:\n",
      "INFO:root:F1_macro  : count: 4.000; mean: 0.894; std: 0.033; min: 0.850; 25%: 0.887; 50%: 0.899; 75%: 0.907; max: 0.929\n",
      "INFO:root:F1        : count: 4.000; mean: 0.926; std: 0.022; min: 0.896; 25%: 0.920; 50%: 0.928; 75%: 0.934; max: 0.951\n",
      "INFO:root:F1-neg    : count: 4.000; mean: 0.863; std: 0.043; min: 0.804; 25%: 0.853; 50%: 0.869; 75%: 0.879; max: 0.908\n",
      "INFO:root:ACC       : count: 4.000; mean: 0.904; std: 0.029; min: 0.865; 25%: 0.897; 50%: 0.907; 75%: 0.915; max: 0.936\n",
      "INFO:root:prec      : count: 4.000; mean: 0.919; std: 0.024; min: 0.885; 25%: 0.915; 50%: 0.926; 75%: 0.930; max: 0.940\n",
      "INFO:root:recall    : count: 4.000; mean: 0.933; std: 0.022; min: 0.909; 25%: 0.925; 50%: 0.931; 75%: 0.939; max: 0.962\n",
      "INFO:root:MCC       : count: 4.000; mean: 0.789; std: 0.065; min: 0.701; 25%: 0.773; 50%: 0.798; 75%: 0.813; max: 0.859\n",
      "INFO:root:Per-fold scores:\n",
      "INFO:root:F1_macro  : [0.929, 0.850, 0.899, 0.899]\n",
      "INFO:root:F1        : [0.951, 0.896, 0.928, 0.929]\n",
      "INFO:root:F1-neg    : [0.908, 0.804, 0.869, 0.869]\n",
      "INFO:root:ACC       : [0.936, 0.865, 0.907, 0.907]\n",
      "INFO:root:prec      : [0.940, 0.885, 0.927, 0.925]\n",
      "INFO:root:recall    : [0.962, 0.909, 0.930, 0.932]\n",
      "INFO:root:MCC       : [0.859, 0.701, 0.798, 0.797]\n",
      "INFO:root:Confusion matrix:\n",
      "INFO:root: 604.25,  106.00\n",
      "INFO:root:  86.25, 1203.50\n"
     ]
    }
   ],
   "source": [
    "_ = run_classif_crossvalid(\n",
    "    'combined',\n",
    "    'cardiffnlp/twitter-xlm-roberta-large-2022',\n",
    "    model_params,\n",
    "    positive_class='critical',\n",
    "    num_folds=4,\n",
    "    rnd_seed=3154561,\n",
    "    test=False,\n",
    "    pause_after_fold=0,\n",
    "    augment_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

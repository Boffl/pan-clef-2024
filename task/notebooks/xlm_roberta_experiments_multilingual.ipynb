{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabple warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/git/pan-clef-2024/task\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (3.7.4)\n",
      "Requirement already satisfied: pynvml in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (11.5.0)\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (4.40.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (2.3.0a0+gitae01701)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->transformers[torch]) (2.8.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy pynvml transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 22:47:05.900057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from classif_experim.classif_experiment_runner import (\n",
    "    run_classif_crossvalid,\n",
    "    run_classif_experiments\n",
    ")\n",
    "import logging, classif_experim\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,  # Log INFO level and above\n",
    "        handlers=[logging.StreamHandler()]  # Log to console\n",
    "    )\n",
    "classif_experim.classif_experiment_runner.logger = logging.getLogger('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter-XLM-Roberta-base\n",
    "[Source](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base)\n",
    "\n",
    "Setup:\n",
    " - Language: English and Spanish combined\n",
    " - No data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_train_epochs': 3,\n",
    "    'warmup': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'batch_size': 16,\n",
    "    'lang': 'combined',\n",
    "    'eval': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUNNING crossvalid. for model: cardiffnlp/twitter-xlm-roberta-base, augment_data=False\n",
      "INFO:root:Starting Fold 1\n",
      "INFO:root:model built\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON combined dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpqiynwow1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2a64b5d4cf4aa5b6b33c33c24b490b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab22200d37d44f7aab0a08d32c64551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 05:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495400</td>\n",
       "      <td>0.433115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.378848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.467284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 1 scores:\n",
      "INFO:root:F1_macro  : 0.834; F1        : 0.893; F1-neg    : 0.775; ACC       : 0.855; prec      : 0.851; recall    : 0.940; MCC       : 0.678\n",
      "INFO:root:Starting Fold 2\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmp2rmiumkt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcd07da03dc4c6eb6c56cb38bee42b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1d3401d8414028b4fac264f285e7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 05:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.365869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326300</td>\n",
       "      <td>0.284520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.292226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 2 scores:\n",
      "INFO:root:F1_macro  : 0.820; F1        : 0.882; F1-neg    : 0.758; ACC       : 0.841; prec      : 0.848; recall    : 0.919; MCC       : 0.646\n",
      "INFO:root:Starting Fold 3\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmprwr72hm3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870bebe27be848108564dca8345be541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e67fb16498b481280d73e94cf358666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 05:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.361813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>0.302110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.321572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 3 scores:\n",
      "INFO:root:F1_macro  : 0.839; F1        : 0.896; F1-neg    : 0.782; ACC       : 0.859; prec      : 0.856; recall    : 0.939; MCC       : 0.686\n",
      "INFO:root:Starting Fold 4\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpsubnyupw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7419441af828422aa5e6f8fb7b507ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db7aa66e07c44ff83b72b431d6260f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 05:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.516084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.381953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.430599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 4 scores:\n",
      "INFO:root:F1_macro  : 0.859; F1        : 0.902; F1-neg    : 0.816; ACC       : 0.872; prec      : 0.893; recall    : 0.911; MCC       : 0.718\n",
      "INFO:root:Starting Fold 5\n",
      "INFO:root:model built\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpasr46vab\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a747e0c1e7794e36a29b652a3647ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d977ccf5a0a4ca084b4429b4d5fc6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 05:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.345025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.405313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.372035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 5 scores:\n",
      "INFO:root:F1_macro  : 0.833; F1        : 0.888; F1-neg    : 0.777; ACC       : 0.851; prec      : 0.861; recall    : 0.918; MCC       : 0.669\n",
      "INFO:root:CROSSVALIDATION results:\n",
      "INFO:root:F1_macro  : count: 5.000; mean: 0.837; std: 0.014; min: 0.820; 25%: 0.833; 50%: 0.834; 75%: 0.839; max: 0.859\n",
      "INFO:root:F1        : count: 5.000; mean: 0.892; std: 0.007; min: 0.882; 25%: 0.888; 50%: 0.893; 75%: 0.896; max: 0.902\n",
      "INFO:root:F1-neg    : count: 5.000; mean: 0.782; std: 0.021; min: 0.758; 25%: 0.775; 50%: 0.777; 75%: 0.782; max: 0.816\n",
      "INFO:root:ACC       : count: 5.000; mean: 0.856; std: 0.011; min: 0.841; 25%: 0.851; 50%: 0.855; 75%: 0.859; max: 0.872\n",
      "INFO:root:prec      : count: 5.000; mean: 0.862; std: 0.018; min: 0.848; 25%: 0.851; 50%: 0.856; 75%: 0.861; max: 0.893\n",
      "INFO:root:recall    : count: 5.000; mean: 0.925; std: 0.013; min: 0.911; 25%: 0.918; 50%: 0.919; 75%: 0.939; max: 0.940\n",
      "INFO:root:MCC       : count: 5.000; mean: 0.679; std: 0.026; min: 0.646; 25%: 0.669; 50%: 0.678; 75%: 0.686; max: 0.718\n",
      "INFO:root:Per-fold scores:\n",
      "INFO:root:F1_macro  : [0.834, 0.820, 0.839, 0.859, 0.833]\n",
      "INFO:root:F1        : [0.893, 0.882, 0.896, 0.902, 0.888]\n",
      "INFO:root:F1-neg    : [0.775, 0.758, 0.782, 0.816, 0.777]\n",
      "INFO:root:ACC       : [0.855, 0.841, 0.859, 0.872, 0.851]\n",
      "INFO:root:prec      : [0.851, 0.848, 0.856, 0.893, 0.861]\n",
      "INFO:root:recall    : [0.940, 0.919, 0.939, 0.911, 0.918]\n",
      "INFO:root:MCC       : [0.678, 0.646, 0.686, 0.718, 0.669]\n",
      "INFO:root:Confusion matrix:\n",
      "INFO:root: 414.40,  153.80\n",
      "INFO:root:  77.20,  954.60\n"
     ]
    }
   ],
   "source": [
    "_ = run_classif_crossvalid(\n",
    "    'combined',\n",
    "    'cardiffnlp/twitter-xlm-roberta-base',\n",
    "    model_params,\n",
    "    positive_class='critical',\n",
    "    num_folds=5,\n",
    "    rnd_seed=3154561,\n",
    "    test=False,\n",
    "    pause_after_fold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter-XLM-Roberta-base\n",
    "[Source](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base)\n",
    "\n",
    "Setup:\n",
    " - Language: English and Spanish combined\n",
    " - WITH augmentated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_classif_crossvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_train_epochs': 3,\n",
    "    'warmup': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'batch_size': 16,\n",
    "    'lang': 'combined',\n",
    "    'eval': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUNNING crossvalid. for model: cardiffnlp/twitter-xlm-roberta-base, augment_data=True\n",
      "INFO:root:Starting Fold 1\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON combined dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpxenkyck5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a83bfcce89b44d8a363d8641eac7c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e67db42707d42cb849ea98187dc178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.307546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.286419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.176300</td>\n",
       "      <td>0.349727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 1 scores:\n",
      "INFO:root:F1_macro  : 0.882; F1        : 0.916; F1-neg    : 0.847; ACC       : 0.892; prec      : 0.914; recall    : 0.919; MCC       : 0.764\n",
      "INFO:root:Starting Fold 2\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpo5pk90am\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79896d41998a4aa2bc9e8d2d8ae41ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006f2c0846104c65a53a0de4ff739b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.305901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.271724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.346142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 2 scores:\n",
      "INFO:root:F1_macro  : 0.870; F1        : 0.906; F1-neg    : 0.834; ACC       : 0.880; prec      : 0.916; recall    : 0.896; MCC       : 0.741\n",
      "INFO:root:Starting Fold 3\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmplv_1o9sq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4136d3187d540f68a45e4c58342659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855bac85220a44ea85a06b2d4f0d90a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.327669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.286200</td>\n",
       "      <td>0.328151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.375632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 3 scores:\n",
      "INFO:root:F1_macro  : 0.856; F1        : 0.898; F1-neg    : 0.814; ACC       : 0.868; prec      : 0.898; recall    : 0.897; MCC       : 0.712\n",
      "INFO:root:Starting Fold 4\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmp46dcfp67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0bd250028d48f08697410da2cddff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2cec40b96349a08920fdc9d7377eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.357390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.325571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.385323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 4 scores:\n",
      "INFO:root:F1_macro  : 0.878; F1        : 0.912; F1-neg    : 0.843; ACC       : 0.887; prec      : 0.918; recall    : 0.907; MCC       : 0.756\n",
      "INFO:root:Starting Fold 5\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpf1r0dd5d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eacc564991f477d9f34cf559eb8293a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b204cc749144308ec4d77aa33fcec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.324948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.308265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.403099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 5 scores:\n",
      "INFO:root:F1_macro  : 0.874; F1        : 0.916; F1-neg    : 0.833; ACC       : 0.888; prec      : 0.888; recall    : 0.946; MCC       : 0.752\n",
      "INFO:root:CROSSVALIDATION results:\n",
      "INFO:root:F1_macro  : count: 5.000; mean: 0.872; std: 0.010; min: 0.856; 25%: 0.870; 50%: 0.874; 75%: 0.878; max: 0.882\n",
      "INFO:root:F1        : count: 5.000; mean: 0.910; std: 0.008; min: 0.898; 25%: 0.906; 50%: 0.912; 75%: 0.916; max: 0.916\n",
      "INFO:root:F1-neg    : count: 5.000; mean: 0.834; std: 0.013; min: 0.814; 25%: 0.833; 50%: 0.834; 75%: 0.843; max: 0.847\n",
      "INFO:root:ACC       : count: 5.000; mean: 0.883; std: 0.009; min: 0.868; 25%: 0.880; 50%: 0.887; 75%: 0.888; max: 0.892\n",
      "INFO:root:prec      : count: 5.000; mean: 0.907; std: 0.013; min: 0.888; 25%: 0.898; 50%: 0.914; 75%: 0.916; max: 0.918\n",
      "INFO:root:recall    : count: 5.000; mean: 0.913; std: 0.020; min: 0.896; 25%: 0.897; 50%: 0.907; 75%: 0.919; max: 0.946\n",
      "INFO:root:MCC       : count: 5.000; mean: 0.745; std: 0.020; min: 0.712; 25%: 0.741; 50%: 0.752; 75%: 0.756; max: 0.764\n",
      "INFO:root:Per-fold scores:\n",
      "INFO:root:F1_macro  : [0.882, 0.870, 0.856, 0.878, 0.874]\n",
      "INFO:root:F1        : [0.916, 0.906, 0.898, 0.912, 0.916]\n",
      "INFO:root:F1-neg    : [0.847, 0.834, 0.814, 0.843, 0.833]\n",
      "INFO:root:ACC       : [0.892, 0.880, 0.868, 0.887, 0.888]\n",
      "INFO:root:prec      : [0.914, 0.916, 0.898, 0.918, 0.888]\n",
      "INFO:root:recall    : [0.919, 0.896, 0.897, 0.907, 0.946]\n",
      "INFO:root:MCC       : [0.764, 0.741, 0.712, 0.756, 0.752]\n",
      "INFO:root:Confusion matrix:\n",
      "INFO:root: 471.00,   97.20\n",
      "INFO:root:  89.80,  942.00\n"
     ]
    }
   ],
   "source": [
    "_ = run_classif_crossvalid(\n",
    "    'combined',\n",
    "    'cardiffnlp/twitter-xlm-roberta-base',\n",
    "    model_params,\n",
    "    positive_class='critical',\n",
    "    num_folds=5,\n",
    "    rnd_seed=3154561,\n",
    "    test=False,\n",
    "    pause_after_fold=0,\n",
    "    augment_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_classif_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUNNING classif. experiments: lang=COMBINED, num_folds=5, max_seq_len=256, eval=0.1, rnd_seed=3154561, test=False\n",
      "INFO:root:... HPARAMS = learning_rate: 2e-05; num_train_epochs: 3; warmup: 0.1; weight_decay: 0.01; batch_size: 16\n",
      "INFO:root:RUNNING crossvalid. for model: cardiffnlp/twitter-xlm-roberta-base, augment_data=True\n",
      "INFO:root:Starting Fold 1\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON combined dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpwayed1q7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49857a2a57c3498482eeb9b9d9789b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b174f9c6921e4af7943c8d8f3de95521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.307546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.286419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.176300</td>\n",
       "      <td>0.349727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 1 scores:\n",
      "INFO:root:F1_macro  : 0.882; F1        : 0.916; F1-neg    : 0.847; ACC       : 0.892; prec      : 0.914; recall    : 0.919; MCC       : 0.764\n",
      "INFO:root:Starting Fold 2\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmps2lvma0_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562c54647c474931ad7f3a68f8220281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0937b5a7c0c744a1b1fc52b1e080a1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.305901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.271724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.346142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 2 scores:\n",
      "INFO:root:F1_macro  : 0.870; F1        : 0.906; F1-neg    : 0.834; ACC       : 0.880; prec      : 0.916; recall    : 0.896; MCC       : 0.741\n",
      "INFO:root:Starting Fold 3\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmp7g4o4rjs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c628a7ecaf74a429e664342233811b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e526967105e942de97e8191abf414b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.327669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.286200</td>\n",
       "      <td>0.328151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.375632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 3 scores:\n",
      "INFO:root:F1_macro  : 0.856; F1        : 0.898; F1-neg    : 0.814; ACC       : 0.868; prec      : 0.898; recall    : 0.897; MCC       : 0.712\n",
      "INFO:root:Starting Fold 4\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmp5dn2byy0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78fe24576ea4d618ae96d48bab2e8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fa243a6675403d9ec128b59aae888b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.357390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.325571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.385323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 4 scores:\n",
      "INFO:root:F1_macro  : 0.878; F1        : 0.912; F1-neg    : 0.843; ACC       : 0.887; prec      : 0.918; recall    : 0.907; MCC       : 0.756\n",
      "INFO:root:Starting Fold 5\n",
      "INFO:root:model built\n",
      "INFO:root:Running on augmented data. Length of the Training set: 13950, 13950\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder: /tmp/tmpjjrrcfpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4821e27e4394a50abe11cfa5206933a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982b6c71985d4ce8af61473c1df1ba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2355' max='2355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2355/2355 10:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.324948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.307748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.403230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fold 5 scores:\n",
      "INFO:root:F1_macro  : 0.875; F1        : 0.916; F1-neg    : 0.834; ACC       : 0.889; prec      : 0.889; recall    : 0.946; MCC       : 0.754\n",
      "INFO:root:CROSSVALIDATION results:\n",
      "INFO:root:F1_macro  : count: 5.000; mean: 0.872; std: 0.010; min: 0.856; 25%: 0.870; 50%: 0.875; 75%: 0.878; max: 0.882\n",
      "INFO:root:F1        : count: 5.000; mean: 0.910; std: 0.008; min: 0.898; 25%: 0.906; 50%: 0.912; 75%: 0.916; max: 0.916\n",
      "INFO:root:F1-neg    : count: 5.000; mean: 0.835; std: 0.013; min: 0.814; 25%: 0.834; 50%: 0.834; 75%: 0.843; max: 0.847\n",
      "INFO:root:ACC       : count: 5.000; mean: 0.883; std: 0.010; min: 0.868; 25%: 0.880; 50%: 0.887; 75%: 0.889; max: 0.892\n",
      "INFO:root:prec      : count: 5.000; mean: 0.907; std: 0.013; min: 0.889; 25%: 0.898; 50%: 0.914; 75%: 0.916; max: 0.918\n",
      "INFO:root:recall    : count: 5.000; mean: 0.913; std: 0.020; min: 0.896; 25%: 0.897; 50%: 0.907; 75%: 0.919; max: 0.946\n",
      "INFO:root:MCC       : count: 5.000; mean: 0.745; std: 0.020; min: 0.712; 25%: 0.741; 50%: 0.754; 75%: 0.756; max: 0.764\n",
      "INFO:root:Per-fold scores:\n",
      "INFO:root:F1_macro  : [0.882, 0.870, 0.856, 0.878, 0.875]\n",
      "INFO:root:F1        : [0.916, 0.906, 0.898, 0.912, 0.916]\n",
      "INFO:root:F1-neg    : [0.847, 0.834, 0.814, 0.843, 0.834]\n",
      "INFO:root:ACC       : [0.892, 0.880, 0.868, 0.887, 0.889]\n",
      "INFO:root:prec      : [0.914, 0.916, 0.898, 0.918, 0.889]\n",
      "INFO:root:recall    : [0.919, 0.896, 0.897, 0.907, 0.946]\n",
      "INFO:root:MCC       : [0.764, 0.741, 0.712, 0.756, 0.754]\n",
      "INFO:root:Confusion matrix:\n",
      "INFO:root: 471.20,   97.00\n",
      "INFO:root:  89.80,  942.00\n"
     ]
    }
   ],
   "source": [
    "_ = run_classif_experiments(\n",
    "    lang='combined',\n",
    "    rnd_seed=3154561,\n",
    "    num_folds=5,\n",
    "    pause_after_fold=0,\n",
    "    augment_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

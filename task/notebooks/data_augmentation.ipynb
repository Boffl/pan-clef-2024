{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helsinki-NLP/opus-mt-en-es & es-en\n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-es-en  \n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-en-es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# English to Spanish\n",
    "tokenizer_en_es = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "model_en_es = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "# Spanish to English\n",
    "tokenizer_es_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "model_es_en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Your Spanish text here.\n"
     ]
    }
   ],
   "source": [
    "# Define your Spanish text\n",
    "spanish_text = \"Tu texto en español aquí.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer_es_en(spanish_text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform translation\n",
    "outputs = model_es_en.generate(**inputs)\n",
    "\n",
    "# Decode the generated output\n",
    "translated_text = tokenizer_es_en.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish train data (to English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nadia Timoleon\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_to_df(data_type):\n",
    "  pickle_path = \"./Documents/GitHub/pan-clef-2024/data/pickle/\"\n",
    "  data_path = os.path.join(pickle_path, data_type)\n",
    "  df = pd.read_pickle(data_path)\n",
    "  df = df[['id', 'text']].copy() # select only the 'id' and 'text' columns\n",
    "  # df.set_index('id', inplace=True) # set 'id' column as index\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, tokenizer, model):\n",
    "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "  outputs = model.generate(**inputs)\n",
    "  translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_es = load_pickle_to_df('dataset_es_train.pkl')\n",
    "df_train_es.rename(columns={'text': 'text_es'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crashes! Sequence too long\n",
    "df_train_es['text_en'] = df_train_es['text'].apply(translate_text, tokenizer=tokenizer_es_en, model=model_es_en)\n",
    "\n",
    "df_train_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 813\n",
      "Translated text: The vaccine is to blame for the increase in cases : clear conclusion when seeing the graphs in these countries : in the case of Nigeria it is very clear, in Ethiopia the same happens : they start to vaccinate and the cases rise exponentially. In Ethiopia less than 2% of the population has the two doses. The cases increased a few weeks after the vaccination increased there And now I ask you : if the vaccine does not stop anything but rather increases the number of positives, the passport COVID What is it? What is it for? What guarantees does it have? Why do the vaccinated get sick more? Guarantee to get sick? Why restrict rights to the non-VACONEED if we also know that the Vaccination DOES NOT SINCE? If you are also against the passport covid us in : t. me / NEWS _ DISIDENTES\n"
     ]
    }
   ],
   "source": [
    "# Average-sized sequence\n",
    "text = df_train_es['text_es'].iloc[390]\n",
    "print(\"Text length:\", len(text))\n",
    "translated_text = translate_text(text, tokenizer=tokenizer_es_en, model=model_es_en)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 1631\n",
      "Translated text: By dismounting the lies of Risto Mejide. The vaccinated people get sick more and die more in relation to covid. From Report 508 of the Ministry of Health? it is deduced that 65.1% of those hospitalized, 50.8% of those admitted to UCI, and 77.1% of the deceased were fully vaccinated. And the difference is still much greater than that reflected by this data, as this report states, I copy textually \" A person is considered fully vaccinated 7 days after receiving a second dose of Comirnaty ( Pfizer / BioNTech ) or 14 days after the second dose of Vaxzevria ( Oxford / AstraZeneca ) or Moderna, and if between the first and second dose there has been a minimum interval of 19 days if the first dose of Comurnaty ( Pfizer / BioNTech ) or 21 days after the second dose of Vaxzevria or 25 days after the second dose of Moderna. Also is considered fully vaccinated a person 14 days after the first and second dose of the second dose of vaccine has been given of Janssen and if the first dose has been given at least if the first dose of the first dose of Janssen and the first dose if the first dose was not if the first dose was of Comur and the first dose was not at least if the first dose was from Comur if the first dose, 21 if the first dose, 21 days when the first dose was born, if the first dose was from Comur, if the first dose, 21, or 65 was born, 21, 21, if the first, or the first dose was not, 21, or the first, 21 days if the first dose was not, if the first dose was, 21 days if the first dose was, 21 days if the first, or the first dose, or the first dose was not, or the first dose was not, or the first dose was, or the first dose was, 21 days if the first dose was not, or the second, or the first, or the second dose was not, or the first dose was not, or the first, or the first dose was not, or the first, or the second dose was not,\n"
     ]
    }
   ],
   "source": [
    "# Sequence with length > mean + std\n",
    "text = df_train_es['text_es'].iloc[1372]\n",
    "print(\"Text length:\", len(text))\n",
    "translated_text = translate_text(text, tokenizer=tokenizer_es_en, model=model_es_en)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pan_clef_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
